---
title: "Tutorial"
author: "Rongqian Zhang"
output: 
  html_document:
    theme: spacelab
    highlight: tango
    includes:
    toc: true
    number_sections: true
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
    code_folding: show
vignette: >
  %\VignetteIndexEntry{Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
## Introduction

`RegularizedRegression` is a package that fits regularized linear models (Ridge Regression & LASSO Regression). It can efficiently estimate the  coefficients given the design matrix, response and regularization parameter. 

This vignette describes the usage of `RegularizedRegression` in R. It also compares the performance of this package with other packages, such as `glmnet` and `MASS`.

# Installation

```{r, warning=FALSE,message=FALSE}
install.packages('devtools',repos = 'https://cran.r-project.org/')
```


```{r, warning=FALSE,message=FALSE}
devtools::install_github("qinyiyi7/RegularizedRegression")
install.packages('glmnet',repos = 'https://cran.r-project.org/')
install.packages('bench',repos = 'https://cran.r-project.org/')
install.packages('ggbeeswarm',repos = 'https://cran.r-project.org/')
```

# Quick Start

First, we load the `RegularizedRegression` and `glmnet` package:
```{r, warning=FALSE,message=FALSE}
library(RegularizedRegression)
library(glmnet)
```

## RidgeReg()

```{r}
data(longley)
```

The command loads a macroeconomic data set which provides a well-known example for a highly collinear regression from this saved R data archive.

### Fit Ridge Regression

We fit the model (GNP.deflator ~ GNP+Unemployed+Armed.Forces+Population+Year+Employed) using different lambda

```{r}
lambda=seq(from=0,to=0.1,by=0.01)
beta_hat=matrix(nrow=6,ncol = length(lambda))
for (i in 1:length(lambda))
{
  beta_hat[,i]=RidgeReg(scale(longley[,2:7]),longley$y,lambda = lambda[i])
}
```

Then, we can visualize the coefficients by executing the `plot.RegPath` function:

```{r}
plot_RegPath(lambda,beta_hat)
```

Each curve corresponds to a variable. It shows the path of its coefficient against the $\lambda$. When $\lambda$ increases, all the coefficients shrink to 0.

### Comparison with glmnet()

Finally, we can compare our function to glmnet() function in `glmnet`.

```{r}


result<-glmnet(scale(as.matrix(longley[,2:7])), longley$y, alpha = 0,  lambda = seq(0,0.1,0.01),thresh = 1e-100)
# plot(lm.ridge(y ~ ., longley,
#               lambda = seq(0,0.1,0.001)))
```

```{r}
glmnet_beta<-as.matrix(result[["beta"]])[,11:1]
plot_RegPath(lambda,glmnet_beta)
```

After comparing these two regularization path of coefficients against lambda, we can conclude that the results of these two functions are very similar.

Furthermore, we can use all.equal() to show the correctness of RidgeReg().

```{r}
all.equal(as.vector(beta_hat),as.vector(glmnet_beta),tolerance=0.15)
```


In addtion, we can compare the efficiency of these functions. 

```{r, warning=FALSE,message=FALSE}
library('bench')
library('ggbeeswarm')
result=bench::mark(RidgeReg(scale(longley[,2:7]),longley$y,lambda = 0.1),glmnet(scale(as.matrix(longley[,2:7])), longley$y, alpha = 0,  lambda = 0.1,thresh = 1e-100),check=FALSE)
print(result)
plot(result)
```


Therefore, we can clearly see that RidgeReg() is much more efficient than glmnet().

## LassoReg()

```{r}
data(sim_data)
```

The command loads a simulated data set which includes repsonse and 10 predictors.


### Fit Lasso Regression

Again, we fit the model (y ~ X) using different lambda

```{r}

lambda = seq(0, 1, by = .1)
beta_hat=matrix(nrow=10,ncol = length(lambda))
for (i in 1:length(lambda))
{
  beta_hat[,i]=LassoReg(X, y,lambda = lambda[i],thresh = 1e-18,soft=TRUE)
}
```


```{r}
plot_RegPath(lambda,beta_hat)
```

```{r}
result<-glmnet(X, y, alpha = 1,  lambda =   seq(0, 1, by = .1),thresh = 1e-12)
```


```{r}
lambda = seq(0, 1, by = .1)
glmnet_beta<-as.matrix(result[["beta"]])[,length(lambda):1]
plot_RegPath(lambda,glmnet_beta)
```

After comparing these two regularization path of coefficients against lambda, we can conclude that the results of these two functions are very similar.

Furthermore, we can use all.equal() to show the correctness of LassoReg().

```{r}
all.equal(as.vector(beta_hat),as.vector(glmnet_beta),tolerance=0.01)
```

In addtion, we can compare the efficiency of these functions. 

```{r, warning=FALSE,message=FALSE}

result=bench::mark(LassoReg(X, y,lambda = 0.1,thresh = 1e-12,soft=TRUE),glmnet(X, y, alpha = 1,  lambda = 0.1,thresh = 1e-12),check=FALSE)
print(result)
plot(result)
```

Therefore, we can clearly see that LassoReg() is less efficient than glmnet().
